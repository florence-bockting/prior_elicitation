{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da34bd3e-821a-42ac-ba6c-13d35331582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import elicit as el\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9256ad6-b002-4aed-9986-1aa63e75bf5e",
   "metadata": {},
   "source": [
    "# Specify the generative model\n",
    "\n",
    "The generative model must be a ``class callable`` with a specific *input-output-structure*.\n",
    "\n",
    "**Input** structure:\n",
    "+ necessary inputs: \n",
    "    + `prior_samples`\n",
    "    + (if `tf.utils.softmax_gumbel_trick` is used) `**kwargs`\n",
    "+ optional inputs\n",
    "    + any arguments that are required by the internal computation of the generative model (e.g., design matrix, N)\n",
    "    + the optional arguments have to be specified as keyword arguments in `el.model`\n",
    "\n",
    "Note: `tf.utils.softmax_gumbel_trick` is required when a discrete likelihood is used (see [Example 2](#example-2:-discrete-likelihood)).\n",
    " \n",
    "**Output** structure:\n",
    "+ necessary output format: *dictionary*\n",
    "    + *keys*: refer to names of target quantities (as defined in section *parameters* in `el.Elicit`) \n",
    "    + *values*: corresponding `tf.Tensor` as computed by the generative model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32284753-3af4-40ed-82b1-c59c3f8e213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo Code\n",
    "\n",
    "class GenerativeModel:\n",
    "    def __call__(self, prior_samples, **kwargs):\n",
    "        \n",
    "        # specify here your generative process: which takes the samples \n",
    "        # from the prior as input and computes the target quantities \n",
    "        # (e.g., observations from the outcome variable)\n",
    "        target = ...\n",
    "        \n",
    "        # return the computed target quantities which \n",
    "        # should be queried from the domain expert\n",
    "        return dict(target = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6973e-73ea-4628-8a34-67ee0baa0210",
   "metadata": {},
   "source": [
    "## Example 1: Continuous likelihood\n",
    "\n",
    "### Probabilistic model notation\n",
    "\\begin{align*}\n",
    "    (\\beta_0, \\beta_1) &\\sim \\text{Normal}(\\mu_i, \\sigma_i) \\quad \\text{ for } i=0,1 \\\\\n",
    "    \\boldsymbol{\\mu} &= \\boldsymbol{\\beta}\\textbf{X}^\\top \\\\\n",
    "    \\textbf{y}_{pred} &\\sim \\text{Normal}(\\boldsymbol{\\mu}, \\textbf{1.})\n",
    "\\end{align*}\n",
    "\n",
    "### Target quantities\n",
    "We assume, we want to query the domain expert regarding the outcome variable.\n",
    "Specifically, we ask the expert for two values of the predictor variable:\n",
    "+ $y_{pred} \\mid X_{1}$\n",
    "+ $y_{pred} \\mid X_{2}$\n",
    "+ $y_{pred} \\mid X_{3}$\n",
    "\n",
    "Let the corresponding design matrix be\n",
    "\n",
    "$\n",
    "\\textbf{X}=\\begin{bmatrix} 1. & -1 \\\\ 1. & 0. \\\\ 1. & 1. \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Computational model implementation\n",
    "#### Create a Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1e394942-1d86-4acd-a031-863b5588c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModel1:\n",
    "    def __call__(self, prior_samples, X):\n",
    "\n",
    "        # shape=(B,num_samples,num_obs)\n",
    "        mu=tf.matmul(prior_samples, X, transpose_b=True) \n",
    "\n",
    "        # shape=(B,num_samples,num_obs)\n",
    "        ypred=tfd.Normal(mu, 1.).sample() \n",
    "\n",
    "        return dict(y_X1=ypred[:,:,0], # shape=(B,num_samples)\n",
    "                    y_X2=ypred[:,:,1], # shape=(B,num_samples)\n",
    "                    y_X3=ypred[:,:,2]) # shape=(B,num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd34376-a6e3-4f8a-9f99-fdd6028426ae",
   "metadata": {},
   "source": [
    "#### Look behind the scenes\n",
    "Let us use the `ExampleModel1` and generate some predictions based on artificial prior distributions. \n",
    "\n",
    "1. Draw prior samples from the following \"ground truth\": $\\beta_0 \\sim \\text{Normal}(1., 0.8)$ and $\\beta_1 \\sim \\text{Normal}(2., 1.5)$\n",
    "2. Define the design matrix $\\textbf{X}$\n",
    "3. Instantiate the generative model\n",
    "4. Simulate from the generative model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93e22dbc-5ca7-4ffc-a61e-d5161099579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Step 1) prior samples:\n",
      " tf.Tensor(\n",
      "[[[1.1125073  2.076223  ]\n",
      "  [0.80801105 1.0152982 ]\n",
      "  [0.6078063  2.4104307 ]\n",
      "  [1.039326   1.6489855 ]\n",
      "  [0.65120023 4.5983553 ]\n",
      "  [2.189219   0.14012146]\n",
      "  [1.7928746  2.4565156 ]\n",
      "  [0.6398772  3.0275126 ]\n",
      "  [1.3033034  2.8191433 ]\n",
      "  [0.72945213 2.9439828 ]]], shape=(1, 10, 2), dtype=float32)\n",
      "\n",
      "(Step 2) design matrix:\n",
      " tf.Tensor(\n",
      "[[ 1. -1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  1.]], shape=(3, 2), dtype=float32)\n",
      "\n",
      "(Step 3) instantiated model:\n",
      " <__main__.ExampleModel1 object at 0x000002C263C3E7D0>\n",
      "\n",
      "(Step 4) samples from outcome variable:\n",
      " {'y_X1': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[-1.1919286, -1.800191 , -2.1663888, -1.4374082, -1.8860066,\n",
      "         2.3839483, -2.2876978, -0.7230171,  0.2864045, -3.782659 ]],\n",
      "      dtype=float32)>, 'y_X2': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[-0.1615212 ,  2.1068852 , -0.06249136, -0.03253329,  2.992262  ,\n",
      "         2.6980178 ,  0.5170374 ,  0.67750573,  0.2714877 ,  1.3019307 ]],\n",
      "      dtype=float32)>, 'y_X3': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[2.800105 , 0.6437601, 2.398678 , 3.0435407, 5.7429676, 4.4191847,\n",
      "        1.0006564, 2.4841185, 3.3039265, 3.9367332]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# define number of batches and draws from prior distributions\n",
    "B, num_samples = (1,10)\n",
    "\n",
    "# sample from priors\n",
    "prior_samples = tfd.Normal([1,2], [0.8, 1.5]).sample((B,num_samples))\n",
    "print(\"(Step 1) prior samples:\\n\", prior_samples)\n",
    "\n",
    "# define the design matrix\n",
    "X = tf.constant([[1.,-1.],[1.,0], [1.,1.]])\n",
    "print(\"\\n(Step 2) design matrix:\\n\", X)\n",
    "\n",
    "# create an instance of the generative model class\n",
    "model_instance = ExampleModel1()\n",
    "print(\"\\n(Step 3) instantiated model:\\n\", model_instance)\n",
    "\n",
    "# simulate from the generative model\n",
    "ypred = model_instance(prior_samples, X)\n",
    "print(\"\\n(Step 4) samples from outcome variable:\\n\", ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f5be55-3f69-49c0-99c0-e98b531f3686",
   "metadata": {},
   "source": [
    "#### Implementation in the `eliobj` \n",
    "The corresponding implementation in the `eliobj` would then look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432aaeb5-9054-4b2f-b7bd-6c2cac367ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliobj = el.Elicit(\n",
    "    model=el.model(\n",
    "        obj=ExampleModel1,    # model class \n",
    "        X=X                   # additional input argument for model class\n",
    "        ),\n",
    "    parameters=[\n",
    "        el.parameter(\n",
    "            name=f\"beta{i}\",\n",
    "            family=tfd.Normal,\n",
    "            hyperparams=dict(\n",
    "                loc=el.hyper(f\"mu{i}\"),\n",
    "                scale=el.hyper(f\"sigma{i}\", lower=0)\n",
    "                )\n",
    "        ) for i in range(2)\n",
    "    ],\n",
    "    targets=[\n",
    "        el.target(\n",
    "            name=f\"y_X{i}\",\n",
    "            query=el.queries.quantiles((.05, .25, .50, .75, .95)),\n",
    "            loss=el.losses.MMD2(kernel=\"energy\"),\n",
    "            weight=1.0\n",
    "        ) for i in range(1,4)\n",
    "    ],\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff4bb5-3682-4214-ba63-58fb86436c50",
   "metadata": {},
   "source": [
    "## Example 2: Discrete Likelihood\n",
    "### Important changes to the *continuous* case\n",
    "There are two specific changes in the `class` definition that you need to consider when implementing a discrete likelihood\n",
    "1. The `**kwargs` argument must be included in\n",
    "    + the `__call__` method of your model class\n",
    "    + the `el.utils.softmax_gumbel_trick` function\n",
    "2. The likelihood requires a specific batch shape: `batch_shape=(B,num_samples,num_obs,1)`\n",
    "    + It is sufficient to add this extra dimension to one of the input arguments of your likelihood, as this will automatically propagate to the batch shape of the likelihood.\n",
    "\n",
    "The reason for (1) is that the `el.utils.softmax_gumbel_trick` function involves sampling. To ensure reproducibility, the `seed` information must be passed to this function. By providing the `**kwargs` argument in both the `__call__` method of your class and the `el.utils.softmax_gumbel_trick` function, the `seed` (as specified in the *trainer* section of `el.Elicit`) can be passed internally.\n",
    "While requirement (2) is related to the internal computations performed within the `el.utils.softmax_gumbel_trick` function. \n",
    "\n",
    "### Probabilistic model notation\n",
    "\\begin{align*}\n",
    "    (\\beta_0, \\beta_1) &\\sim \\text{Normal}(\\mu_i, \\sigma_i) \\quad \\text{ for } i=0,1 \\\\\n",
    "    \\boldsymbol{\\theta} &= \\text{sigmoid}(\\boldsymbol{\\beta}\\textbf{X}^\\top) \\\\\n",
    "    \\textbf{z}_{pred} &\\sim \\text{Binomial}(N,\\boldsymbol{\\theta})\n",
    "\\end{align*}\n",
    "\n",
    "### Target quantities\n",
    "We assume, we want to query the domain expert regarding the outcome variable.\n",
    "Specifically, we ask the expert for two values of the predictor variable:\n",
    "+ $z_{pred} \\mid x_{1}$\n",
    "+ $z_{pred} \\mid x_{2}$\n",
    "+ $z_{pred} \\mid x_{3}$\n",
    "\n",
    "Let the corresponding design matrix be\n",
    "\n",
    "$\n",
    "\\textbf{X}=\\begin{bmatrix} 1. & -1 \\\\ 1. & 0. \\\\ 1. & 1. \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Computational model implementation\n",
    "#### Create a Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "16cd2009-0589-4fc9-8cba-7bdb90d7da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModel2:\n",
    "    # Note: pass the **kwargs argument\n",
    "    def __call__(self, prior_samples, X, N, **kwargs):\n",
    "\n",
    "        # shape=(B,num_samples,num_obs)\n",
    "        theta=tf.math.sigmoid(tf.matmul(prior_samples, X, transpose_b=True))\n",
    "\n",
    "        # likelihood; shape=(B,num_samples,num_obs,1)\n",
    "        # Note: add. dim in likelihood required by softmax_gumbel_trick\n",
    "        likelihood=tfd.Binomial(total_count=N, \n",
    "                                probs=tf.expand_dims(theta, axis=-1)\n",
    "                               )\n",
    "        \n",
    "        # approximate samples from the Binomial distribution\n",
    "        # shape=(B,num_samples,num_obs)\n",
    "        # Note: add the **kwargs argument\n",
    "        zpred=el.utils.softmax_gumbel_trick(\n",
    "            epred=theta, likelihood=likelihood, \n",
    "            upper_thres=total_count, \n",
    "            **kwargs)\n",
    "\n",
    "        return dict(z_X1=zpred[:,:,0], # shape=(B,num_samples)\n",
    "                    z_X2=zpred[:,:,1], # shape=(B,num_samples)\n",
    "                    z_X3=zpred[:,:,2]) # shape=(B,num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cecb63-145a-496a-8533-c688013c710b",
   "metadata": {},
   "source": [
    "#### Look behind the scenes\n",
    "Let us use the `ExampleModel2` and generate some predictions based on artificial prior distributions. \n",
    "\n",
    "1. Draw prior samples from the following \"ground truth\": $\\beta_0 \\sim \\text{Normal}(0.1, 0.2)$ and $\\beta_1 \\sim \\text{Normal}(0.2, 0.3)$\n",
    "2. Define the design matrix $\\textbf{X}$\n",
    "3. Instantiate the generative model\n",
    "4. Simulate from the generative model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "45df7daf-3c2d-4197-9429-8258454787ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Step 1) prior samples:\n",
      " tf.Tensor(\n",
      "[[[ 0.18061757 -0.12640624]\n",
      "  [ 0.08738093  0.60096705]\n",
      "  [ 0.24235204  0.05321406]\n",
      "  [-0.05284426 -0.1111746 ]\n",
      "  [-0.15038678  0.20636728]\n",
      "  [-0.01027516 -0.32295096]\n",
      "  [ 0.03292781 -0.11280026]\n",
      "  [ 0.30182764  0.5708762 ]\n",
      "  [-0.03673781  0.40208268]\n",
      "  [ 0.01587544 -0.11239086]]], shape=(1, 10, 2), dtype=float32)\n",
      "\n",
      "(Step 2) design matrix:\n",
      " tf.Tensor(\n",
      "[[ 1. -1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  1.]], shape=(3, 2), dtype=float32)\n",
      "\n",
      "(Step 3) instantiated model:\n",
      " <__main__.ExampleModel2 object at 0x000002C266920550>\n",
      "\n",
      "(Step 4) samples from outcome variable:\n",
      " {'z_X1': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[16.420263, 11.55657 , 17.039097, 14.793844, 12.669424, 16.667715,\n",
      "        17.019955, 13.616165, 11.663627, 16.112122]], dtype=float32)>, 'z_X2': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[16.055965, 17.370132, 16.093798, 15.073592, 13.7894  , 14.599481,\n",
      "        14.547873, 17.074022, 15.857426, 15.451229]], dtype=float32)>, 'z_X3': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[14.718641, 20.798233, 17.821178, 13.658306, 15.893131, 12.64514 ,\n",
      "        14.8872  , 20.550808, 17.580793, 14.436962]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# define number of batches and draws from prior distributions\n",
    "# total_count for Binomial model\n",
    "B, num_samples, N = (1,10,30)\n",
    "\n",
    "# sample from priors\n",
    "prior_samples = tfd.Normal([0.1, 0.2], [0.2, 0.3]).sample((B,num_samples))\n",
    "print(\"(Step 1) prior samples:\\n\", prior_samples)\n",
    "\n",
    "# define the design matrix\n",
    "X = tf.constant([[1.,-1.],[1.,0], [1.,1.]])\n",
    "print(\"\\n(Step 2) design matrix:\\n\", X)\n",
    "\n",
    "# create an instance of the generative model class\n",
    "model_instance = ExampleModel2()\n",
    "print(\"\\n(Step 3) instantiated model:\\n\", model_instance)\n",
    "\n",
    "# simulate from the generative model (specity seed information)\n",
    "zpred = model_instance(prior_samples, X, N, seed=1)\n",
    "print(\"\\n(Step 4) samples from outcome variable:\\n\", zpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290831c0-f759-4547-b5ac-b40fe494a6e4",
   "metadata": {},
   "source": [
    "#### Implementation in the `eliobj` \n",
    "The corresponding implementation in the `eliobj` would then look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7293790-cc70-4845-b08f-2c50b8264acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliobj = el.Elicit(\n",
    "    model=el.model(\n",
    "        obj=ExampleModel2,  # model class \n",
    "        X=X,                # additional input\n",
    "        N=N                 # additional input \n",
    "        ),\n",
    "    parameters=[\n",
    "        el.parameter(\n",
    "            name=f\"beta{i}\",\n",
    "            family=tfd.Normal,\n",
    "            hyperparams=dict(\n",
    "                loc=el.hyper(f\"mu{i}\"),\n",
    "                scale=el.hyper(f\"sigma{i}\", lower=0)\n",
    "                )\n",
    "        ) for i in range(2)\n",
    "    ],\n",
    "    targets=[\n",
    "        el.target(\n",
    "            name=f\"z_X{i}\",\n",
    "            query=el.queries.quantiles((.05, .25, .50, .75, .95)),\n",
    "            loss=el.losses.MMD2(kernel=\"energy\"),\n",
    "            weight=1.0\n",
    "        ) for i in range(1,4)\n",
    "    ],\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b84ed-b937-4842-8166-6f711e545c63",
   "metadata": {},
   "source": [
    "## Notes about tensor operations & shapes\n",
    "+ `prior_samples` have shape `(B, num_samples, num_param)`\n",
    "+ in the notation above `B` refers to the *batch size*, `num_samples` to the number of prior samples, and `num_param` to the number of model parameters\n",
    "+ it is important that you take care about the correct tensor shape, when implementing the generative model\n",
    "\n",
    "### Example for matrix multiplication\n",
    "\n",
    "We assume the following **design matrix**:  \n",
    "$\n",
    "X=\\begin{bmatrix} 1. & -1 \\\\ 1. & 0. \\\\ 1. & 1. \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Then we would get the linear predictor, $\\mu$, as follows:\n",
    "$\n",
    "\\mu = \\boldsymbol{\\beta}\\textbf{X}^\\top\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7902aaac-9f25-4a43-92d7-f2b22f3258c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior samples:\n",
      " tf.Tensor(\n",
      "[[[ 0.21421784  2.1894784 ]\n",
      "  [ 2.0228667   3.8906546 ]\n",
      "  [ 0.14594889  3.722395  ]\n",
      "  [ 1.4024372   1.6433661 ]]\n",
      "\n",
      " [[ 0.8237189   4.390056  ]\n",
      "  [ 1.5101621   2.3289535 ]\n",
      "  [-0.6737598  -0.17162848]\n",
      "  [ 2.4565587  -0.43988562]]], shape=(2, 4, 2), dtype=float32)\n",
      "X:\n",
      " tf.Tensor(\n",
      "[[ 1. -1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  1.]], shape=(3, 2), dtype=float32)\n",
      "mu:\n",
      " tf.Tensor(\n",
      "[[[-1.9752605   0.21421784  2.4036963 ]\n",
      "  [-1.8677878   2.0228667   5.9135213 ]\n",
      "  [-3.576446    0.14594889  3.8683438 ]\n",
      "  [-0.24092889  1.4024372   3.0458033 ]]\n",
      "\n",
      " [[-3.566337    0.8237189   5.213775  ]\n",
      "  [-0.8187914   1.5101621   3.8391156 ]\n",
      "  [-0.50213134 -0.6737598  -0.8453883 ]\n",
      "  [ 2.8964443   2.4565587   2.016673  ]]], shape=(2, 4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# number of batches\n",
    "B=2\n",
    "# number of samples drawn from the prior\n",
    "num_samples=4\n",
    "\n",
    "# prior samples for two model parameters; shape=(B,num_samples,num_param)\n",
    "prior_samples = tfd.Normal([1,2], [0.8, 1.5]).sample((B,num_samples))\n",
    "print(\"prior samples:\\n\", prior_samples)\n",
    "\n",
    "# design matrix;  shape=(num_obs, num_param)\n",
    "X = tf.constant([[1.,-1.],[1.,0], [1.,1.]])\n",
    "print(\"X:\\n\",X)\n",
    "\n",
    "# compute linear predictor mu; \n",
    "# shapes: [B,num_samples,num_param] x [num_params,num_obs] = [B,num_samples,num_obs]\n",
    "mu = tf.matmul(prior_samples, X, transpose_b=True)\n",
    "print(\"mu:\\n\", mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1edc2-ecc1-4f67-bd26-d70ee7685488",
   "metadata": {},
   "source": [
    "### Example for vector multiplication\n",
    "+ consider we want to compute the linear predictor instead as follows:\n",
    "$\n",
    "\\mu = \\beta_0 + \\beta_1x\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f607f59b-9e03-4f04-8bbf-bd250f6e08e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4, 3), dtype=float32, numpy=\n",
       "array([[[-1.9752605 ,  0.21421784,  2.4036963 ],\n",
       "        [-1.8677878 ,  2.0228667 ,  5.9135213 ],\n",
       "        [-3.576446  ,  0.14594889,  3.8683438 ],\n",
       "        [-0.24092889,  1.4024372 ,  3.0458033 ]],\n",
       "\n",
       "       [[-3.566337  ,  0.8237189 ,  5.213775  ],\n",
       "        [-0.8187914 ,  1.5101621 ,  3.8391156 ],\n",
       "        [-0.50213134, -0.6737598 , -0.8453883 ],\n",
       "        [ 2.8964443 ,  2.4565587 ,  2.016673  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the two model parameters\n",
    "b0=prior_samples[:,:,0] # shape=(B,num_samples)\n",
    "b1=prior_samples[:,:,1] # shape=(B,num_samples)\n",
    "\n",
    "# compute linear predictor mu;\n",
    "# shapes: [B,num_samples,1] + [B,num_samples,1] * [1,num_obs] = [B,num_samples,num_obs]\n",
    "# Note: With 'None' we can easily expand a tensor with a new dimension \n",
    "mu = b0[:,:,None]+b1[:,:,None]*X[:,1][None,:] \n",
    "print(\"mu:\\n\", mu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
